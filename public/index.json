[{"content":"\u003c!DOCTYPE html\u003e My Blog ","date":"4 April 2024","externalUrl":null,"permalink":"/","section":"","summary":"\u003c!DOCTYPE html\u003e My Blog ","title":""},{"content":" ","date":"4 April 2024","externalUrl":null,"permalink":"/posts/","section":"Blog","summary":" ","title":"Blog"},{"content":" Motivation # Python is an interpreted language, it means that the source code needs to be executed through an interpreter. As a Python Programmer, understanding the differences between compilers and interpreters is for a better grasp of the underlying techniques of Python and the whole software engineering world.\nFirst, let\u0026rsquo;s talk about Python. It has the following characteristics:\nObject-oriented: it refers to the ability to use encapsulation, inheritance, and polymorphism in program design. Dynamic language: It\u0026rsquo;s a language that can change its structure at runtime; for example, adding attributes to an object of a class that did not exist during program execution. Dynamic typing: Variables do not need to specify a type, but the interpreter needs to identify the data type when executing the code; this feature makes programming simpler, but reduces the efficiency of code execution. High-level language: it refers to programming languages that are highly encapsulated, compared to machine language, they are more suitable for humans to write and read. Interpreted language: it refers to a language that does not need to be compiled and can directly interpret the source code into machine language for execution. compiled languages and interpreted languages # Programming languages are divided into compiled languages and interpreted languages. It\u0026rsquo;s essential to understand their differences to better grasp the distinctions between compilers and interpreters.\nCompiler # As you know, C and C++ are both compiled languages. The characteristic of compiled languages is their fast execution speed. What about the disadvantages, though?\nCompiled languages require a compiler to process them, with the main workflow as follows:\nSource Code → Preprocessor → Compiler → Object Code(Binary Machine Code) → Linker → Executables In this workflow, the compiler invokes the preprocessor for\nPreprocessor: processing, optimizing and transforming the source code, e.g., removing comments, macro definitions, including files, and conditional compilation. Compiler: compiling the preprocessed source code into Object Code (binary machine language), Linker: providing linking with library files, e.g., operating system\u0026rsquo;s API, Finally, it forms an executable program that the machine can execute.\nIn this workflow, the object code must match the machine\u0026rsquo;s CPU architecture, and the library files must match the operating system. If you want to run C language source code on machines with different CPUs or systems, it needs to be compiled for the different CPU architectures and operating systems to run on the machine.\nThus, we see the downside of compiled languages: they are not suitable for cross-platform use. Moreover, that\u0026rsquo;s why an exe program can run on Windows but not on Mac, even if the CPU is the same.\nIf the above feels a bit complex, let me give you an example from everyday life: Imagine a Chinese teacher who teaches many foreign students from the UK, the USA, France, Germany, and South Korea. When this teacher distributes learning materials, they need to first translate the Chinese materials into English, German, French, and Korean electronic documents, then distribute them to students from each country for study. This translation work is very tedious. Not only does it require translation into each country\u0026rsquo;s language, but it also needs to account for the differences between British and American English, creating different versions accordingly. Furthermore, each time the materials are updated, they have to be translated all over again.\nChinese Teacher Metaphor for understanding Compiler Imagine a Chinese teacher who teaches many foreign students from the UK, the USA, France, Germany. When this teacher distributes learning materials, She needs to first translate the Chinese materials into English, German, and French documents, then distribute them to students from each country for study. This translation work is very tedious. Not only does it require translation into each country's language, but it also needs to account for the differences between British and American English, creating different versions accordingly. Furthermore, each time the materials are updated, they have to be translated all over again. In this example, the Chinese teacher is like the developer of a compiled language, the Chinese materials are like the source code of the compiled language, the translated materials are like the machine language for different CPU architectures, and the students speaking different languages are like machines with different CPU architectures. The American and British students are like machines with the same CPU architecture but different operating systems. Interpreter # Metaphor Continued the exhausted Chinese teacher starts to look for a solution. She thinks, why should I do the translating myself instead of giving each student an automatic translation software? So, the teacher provides each student with a custom automatic translation software, which can translate the original Chinese materials page by page into different languages for the students to read. Now, the situation is much easier for the teacher. She doesn't need to consider making materials in various languages anymore and can just focus on creating the Chinese materials. The vanilla/early version of compiler is as\nSource code → interpreter\nThe source code does not need to be compiled into an executable program.\nDuring program execution, the interpreter\nLexical analysis: Breaks down the source code into individual tokens and identifies their categories.\nSyntax analysis: Checks whether the code structure is correct according to the grammar rules and generates a syntax tree.\nSemantic analysis: Checks whether the code semantics are correct and performs type checking and other operations.\nBytecode generation: Generates bytecode from the syntax tree. Bytecode is an intermediate code that the interpreter can understand and execute.\nInterpretation execution:\nSyntax analysis: The interpreter checks whether the syntax of the bytecode is correct. Semantic analysis: The interpreter checks whether the semantics of the bytecode are correct. Bytecode interpretation: The interpreter translates the bytecode into machine instructions. Instruction execution: The CPU executes the translated machine instructions. Interpreted languages have different interpreters on different platforms, so the goal of cross-platform source code is achieved. Developers do not need to consider how to compile on each platform, they only need to focus on writing code, and the written code can be executed correctly on any platform without modification (or minimal modification, e.g., the fork() function is supported in Linux, while in a Windows system, this function is not supported.)\nWhen Python programs run, as in the example above, the source code is first completely transformed and compiled into more efficient bytecode, saved as a “.pyc” bytecode file. Then, the translator executes it line by line translating it into machine language.\nNote: When executing source code in a Shell, no intermediate file is generated; the source code is read, transformed into bytecode, and then interpreted directly.\n","date":"4 April 2024","externalUrl":null,"permalink":"/posts/compiler_vs_interpreter/","section":"Blog","summary":"A brief Overview of Compiler and Interpreter","title":"Compiler_vs_Interpreter"},{"content":"","date":"4 April 2024","externalUrl":null,"permalink":"/tags/engineering/","section":"Tags","summary":"","title":"Engineering"},{"content":"","date":"4 April 2024","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"","date":"12 August 2022","externalUrl":null,"permalink":"/tags/content-creation/","section":"Tags","summary":"","title":"Content Creation"},{"content":"A golden rule in programming is that:\n​\tcode does not do what you expect it to do, but what you tell it to do.\nBridging that gap can sometimes be a quite difficult feat. In this lecture we are going to cover useful techniques for dealing with buggy and resource hungry code: debugging and profiling.\nDebugging # Printf debugging and Logging # \u0026ndash; The most effective debugging tool is still careful thought, coupled with judiciously placed print statements” — Brian Kernighan, Unix for Beginners.\nA first approach to debug a program is to add print statements around where you have detected the problem, and keep iterating until you have extracted enough information to understand what is responsible for the issue.\nA second approach is to use logging in your program, instead of ad hoc print statements. Logging is better than regular print statements for several reasons:\nYou can log to files, sockets or even remote servers instead of standard output. Logging supports severity levels (such as INFO, DEBUG, WARN, ERROR, \u0026amp;c), that allow you to filter the output accordingly. For new issues, there’s a fair chance that your logs will contain enough information to detect what is going wrong. Analysis # For some issues you do not need to run any code. For example, just by carefully looking at a piece of code you could realize that your loop variable is shadowing an already existing variable or function name; or that a program reads a variable before defining it. Here is where static analysis tools come into play. Static analysis programs take source code as input and analyze it using coding rules to reason about its correctness.\nIn the following Python snippet there are several mistakes. First, our loop variable foo shadows the previous definition of the function foo. We also wrote baz instead of bar in the last line, so the program will crash after completing the sleep call (which will take one minute).\nReferences # ","date":"12 August 2022","externalUrl":null,"permalink":"/posts/create-for-whom/","section":"Blog","summary":"A golden rule in programming is that:","title":"Debugging and Profiling"},{"content":"What do we mean by “meta programming”?\nWell, it was the best collective term we could come up with for the set of things that are more about process than they are about writing code or working more efficiently.\nIn this lecture, we will look at systems for building and testing your code, and for managing dependencies.\nBuild systems # If you write a paper in LaTeX, what are the commands you need to run to produce your paper? What about the ones used to run your benchmarks, plot them, and then insert that plot into your paper? Or to compile the code provided in the class you’re taking and then running the tests?\nFor most projects, whether they contain code or not, there is a “build process”. Some sequence of operations you need to do to go from your inputs to your outputs. Often, that process might have many steps, and many branches. Run this to generate this plot, that to generate those results, and something else to produce the final paper. As with so many of the things we have seen in this class, you are not the first to encounter this annoyance, and luckily there exist many tools to help you!\nThese are usually called “build systems”, and there are many of them. Which one you use depends on the task at hand, your language of preference, and the size of the project.\nAt their core, they are all very similar though. You define a number of dependencies, a number of targets, and rules for going from one to the other. You tell the build system that you want a particular target, and its job is to find all the transitive dependencies of that target, and then apply the rules to produce intermediate targets all the way until the final target has been produced. Ideally, the build system does this without unnecessarily executing rules for targets whose dependencies haven’t changed and where the result is available from a previous build.\nmake is one of the most common build systems out there, and you will usually find it installed on pretty much any UNIX-based computer. It has its warts, but works quite well for simple-to-moderate projects. When you run make, it consults a file called Makefile in the current directory. All the targets, their dependencies, and the rules are defined in that file. Let’s take a look at one:\npaper.pdf: paper.tex plot-data.png pdflatex paper.tex plot-%.png: %.dat plot.py ./plot.py -i $*.dat -o $@ right-hand side are dependencies, left-hand side is the target. The indented block is a sequence of programs to produce the target from those dependencies. In make, the first directive also defines the default goal. If you run make with no arguments, this is the target it will build. Alternatively, you can run something like make plot-data.png, and it will build that target instead.\nThe % in a rule is a “pattern”, and will match the same string on the left and on the right. For example, if the target plot-foo.png is requested, make will look for the dependencies foo.dat and plot.py. Now let’s look at what happens if we run make with an empty source directory.\nDependency management # At a more macro level, your software projects are likely to have dependencies that are themselves projects. You might depend on installed programs (like python), system packages (like openssl), or libraries within your programming language (like matplotlib).\nVersioning # The first among these is versioning. Most projects that other projects depend on issue a version number with every release. Usually something like 8.1.3 or 64.1.20192004. They are often, but not always, numerical. Version numbers serve many purposes, and one of the most important of them is to ensure that software keeps working.\nExample:Imagine that I release a new version of my library where I have renamed a particular function. If someone tried to build some software that depends on my library after I release that update, the build might fail because it calls a function that no longer exists!\nVersioning attempts to solve this problem by letting a project say that it depends on a particular version, or range of versions, of some other project. That way, even if the underlying library changes, dependent software continues building by using an older version of my library.\nThat also isn’t ideal though! What if I issue a security update which does not change the public interface of my library (its “API”), and which any project that depended on the old version should immediately start using? This is where the different groups of numbers in a version come in.\nRules for versioning # One common standard is semantic versioning. With semantic versioning, every version number is of the form: major.minor.patch (e.g., 8.1.3). The rules are:\nIf a new release does not change the API, increase the patch version. If you add to your API in a backwards-compatible way, increase the minor version. If you change the API in a non-backwards-compatible way, increase the major version. Now, if my project depends on your project, it should be safe to use the latest release with the same major version as the one I built against when I developed it, as long as its minor version is at least what it was back then. In other words, if I depend on your library at version 1.3.7, then it should be fine to build it with 1.3.8, 1.6.1, or even 1.3.0. Version 2.2.4 would probably not be okay, because the major version was increased. We can see an example of semantic versioning in Python’s version numbers. Many of you are probably aware that Python 2 and Python 3 code do not mix very well, which is why that was a major version bump. Similarly, code written for Python 3.5 might run fine on Python 3.7, but possibly not on 3.4.\nLock files # When working with dependency management systems, you may also come across the notion of lock files. A lock file is simply a file that lists the exact version you are currently depending on of each dependency.\nIn a short lock file is a more professional version of requirement.txt when working in a large project.\nUsually, you need to explicitly run an update program to upgrade to newer versions of your dependencies. There are many reasons for this, such as avoiding unnecessary recompiles, having reproducible builds, or not automatically updating to the latest version (which may be broken). An extreme version of this kind of dependency locking is *vendoring*, which is where you copy all the code of your dependencies into your own project. That gives you total control over any changes to it, and lets you introduce your own changes to it, but also means you have to explicitly pull in any updates from the upstream maintainers over time. Continuous integration (CI) systems # Motivation # As you work on larger and larger projects, you’ll find that there are often additional tasks you have to do whenever you make a change to it. You might have to upload a new version of the documentation, upload a compiled version somewhere, release the code to pypi, run your test suite, and all sort of other things. Maybe every time someone sends you a pull request on GitHub, you want their code to be style checked and you want some benchmarks to run? When these kinds of needs arise, it’s time to take a look at continuous integration.\nWhat is CI and how it works # Continuous integration, or CI, is an umbrella term for “stuff that runs whenever your code changes”, and there are many companies out there that provide various types of CI, often for free for open-source projects. Some of the big ones are Travis CI, Azure Pipelines, and GitHub Actions.\nThey all work in roughly the same way: you add a file to your repository that describes what should happen when various things happen to that repository. By far the most common one is a rule like “when someone pushes code, run the test suite”. When the event triggers, the CI provider spins up a virtual machines (or more), runs the commands in your “recipe”, and then usually notes down the results somewhere. You might set it up so that you are notified if the test suite stops passing, or so that a little badge appears on your repository as long as the tests pass.\nAs an example of a CI system, the class website is set up using GitHub Pages. Pages is a CI action that runs the Jekyll blog software on every push to master and makes the built site available on a particular GitHub domain. This makes it trivial for us to update the website! We just make our changes locally, commit them with git, and then push. CI takes care of the rest.\nCase Study: CI with Github Action # Set up a simple auto-published page using GitHub Pages.\nAdd a GitHub Action to the repository to run shellcheck on any shell files in that repository (here is one way to do it).\nA brief overview on testing # Most large software projects come with a “test suite”. You may already be familiar with the general concept of testing, but we thought we’d quickly mention some approaches to testing and testing terminology that you may encounter in the wild:\nTest suite: a collective term for all the tests Unit test: a “micro-test” that tests a specific feature in isolation Integration test: a “macro-test” that runs a larger part of the system to check that different feature or components work together. Regression test: a test that implements a particular pattern that previously caused a bug to ensure that the bug does not resurface. Mocking: to replace a function, module, or type with a fake implementation to avoid testing unrelated functionality. For example, you might “mock the network” or “mock the disk”. TODO: # In this lecture, it only covers a little bit of flavour of CI, not even CD. A deeper learning about CI/CD is in to-do list.\nReference: # MIT Missing Semesters course https://missing.csail.mit.edu/2020/metaprogramming/\nSome solutions to the homework https://missing-semester-cn.github.io/missing-notes-and-solutions/2020/solutions/metaprogramming-solution/\n","date":"12 August 2022","externalUrl":null,"permalink":"/posts/create-for-whom/","section":"Blog","summary":"What do we mean by “meta programming”?","title":"Meta Programming"},{"content":" Entropy # Entropy is a measure of randomness. This is useful, for example, when determining the strength of a password.\nHash Function # A cryptographic hash function maps data of arbitrary size to a fixed size, and has some special properties. A rough specification of a hash function is as follows:\nhash(value: array\u0026lt;byte\u0026gt;) -\u0026gt; vector\u0026lt;byte, N\u0026gt; (for some fixed N) An example of a hash function is SHA1, which is used in Git. It maps arbitrary-sized inputs to 160-bit outputs (which can be represented as 40 hexadecimal characters). We can try out the SHA1 hash on an input using the sha1sum command:\n$ printf \u0026#39;hello\u0026#39; | sha1sum aaf4c61ddcc5e8a2dabede0f3b482cd9aea9434d $ printf \u0026#39;hello\u0026#39; | sha1sum aaf4c61ddcc5e8a2dabede0f3b482cd9aea9434d $ printf \u0026#39;Hello\u0026#39; | sha1sum f7ff9e8b7bb2e09b70935a5d785e0cc5d9d0abf0 At a high level, a hash function can be thought of as a hard-to-invert random-looking (but deterministic) function (and this is the ideal model of a hash function). A hash function has the following properties:\nDeterministic: the same input always generates the same output. Non-invertible: it is hard to find an input m such that hash(m) = h for some desired output h. Target collision resistant: given an input m_1, it’s hard to find a different input m_2 such that hash(m_1) = hash(m_2). Collision resistant: it’s hard to find two inputs m_1 and m_2 such that hash(m_1) = hash(m_2) (note that this is a strictly stronger property than target collision resistance). Note: while it may work for certain purposes, SHA-1 is no longer considered a strong cryptographic hash function. You might find this table of lifetimes of cryptographic hash functions interesting. However, note that recommending specific hash functions is beyond the scope of this lecture. If you are doing work where this matters, you need formal training in security/cryptography.\nApplications # Git, for content-addressed storage. The idea of a hash function is a more general concept (there are non-cryptographic hash functions). Why does Git use a cryptographic hash function? A short summary of the contents of a file. Software can often be downloaded from (potentially less trustworthy) mirrors, e.g. Linux ISOs, and it would be nice to not have to trust them. The official sites usually post hashes alongside the download links (that point to third-party mirrors), so that the hash can be checked after downloading a file. Commitment schemes. Suppose you want to commit to a particular value, but reveal the value itself later. For example, I want to do a fair coin toss “in my head”, without a trusted shared coin that two parties can see. I could choose a value r = random(), and then share h = sha256(r). Then, you could call heads or tails (we’ll agree that even r means heads, and odd r means tails). After you call, I can reveal my value r, and you can confirm that I haven’t cheated by checking sha256(r) matches the hash I shared earlier. Key derivation functions # A related concept to cryptographic hashes, key derivation functions (KDFs) are used for a number of applications, including producing fixed-length output for use as keys in other cryptographic algorithms. Usually, KDFs are deliberately slow, in order to slow down offline brute-force attacks.\nApplications # Producing keys from passphrases for use in other cryptographic algorithms (e.g. symmetric cryptography, see below). Storing login credentials. Storing plaintext passwords is bad; the right approach is to generate and store a random salt salt = random() for each user, store KDF(password + salt), and verify login attempts by re-computing the KDF given the entered password and the stored salt. Symmetric cryptography # Hiding message contents is probably the first concept you think about when you think about cryptography. Symmetric cryptography accomplishes this with the following set of functionality:\nkeygen() -\u0026gt; key (this function is randomized) encrypt(plaintext: array\u0026lt;byte\u0026gt;, key) -\u0026gt; array\u0026lt;byte\u0026gt; (the ciphertext) decrypt(ciphertext: array\u0026lt;byte\u0026gt;, key) -\u0026gt; array\u0026lt;byte\u0026gt; (the plaintext) The encrypt function has the property that given the output (ciphertext), it’s hard to determine the input (plaintext) without the key. The decrypt function has the obvious correctness property, that decrypt(encrypt(m, k), k) = m.\nAn example of a symmetric cryptosystem in wide use today is AES.\nApplications # Encrypting files for storage in an untrusted cloud service. This can be combined with KDFs, so you can encrypt a file with a passphrase. Generate key = KDF(passphrase), and then store encrypt(file, key). ","date":"12 August 2022","externalUrl":null,"permalink":"/posts/security_and_cryptography/security_and_cryptography/","section":"Blog","summary":"Entropy # Entropy is a measure of randomness.","title":"Security and Cryptography"},{"content":" Purpose # Modern Version Control Systems (VCS) let you easily (and often automatically) answer questions like:\nWho wrote this module?\nWhen was this particular line of this particular file edited? By whom? Why was it edited?\nOver the last 1000 revisions, when/why did a particular unit test stop working?\nGit\u0026rsquo;s data model # snapshot # Git models the history of a collection of files and folders within some top-level directory as a seiries of snapshots.\nGit terminology:\nIn Git, a file is called a \u0026ldquo;blob\u0026rdquo;(a bunch of bytes) directory is called tree, it maps names to blobs or tress (directory can also contain other directories) snapshot: the top-level tree that is being tracked. Below is an example of a tree/snapshot:\n\u0026lt;root\u0026gt; (tree) | +- foo (tree) | | | + bar.txt (blob, contents = \u0026#34;hello world\u0026#34;) | +- baz.txt (blob, contents = \u0026#34;git is wonderful\u0026#34;) This top-level tree contains two elements, a tree “foo” (that itself contains one element, a blob “bar.txt”), and a blob “baz.txt”.\nModeling history: relating snapshots # In Git, a history is a directed acyclic graph (DAG) of snapshots.\nEach snapshot in Git refers to a set of “parents”, the snapshots that preceded it. It’s a set of parents rather than a single parent (as would be the case in a linear history) because a snapshot might descend from multiple parents, for example, due to combining (merging) two parallel branches of development\nGit calls these snapshots “commit”s. Visualizing a commit history might look something like this:\no \u0026lt;-- o \u0026lt;-- o \u0026lt;-- o ^ \\ --- o \u0026lt;-- o In the ASCII art above, the os correspond to individual commits (snapshots). The arrows point to the parent of each commit (it’s a “comes before” relation, not “comes after”). After the third commit, the history branches into two separate branches. This might correspond to, for example, two separate features being developed in parallel, independently from each other. In the future, these branches may be merged to create a new snapshot that incorporates both of the features, producing a new history that looks like this, with the newly created merge commit shown in bold:\no \u0026lt;-- o \u0026lt;-- o \u0026lt;-- o \u0026lt;---- **o** ^ / \\ v --- o \u0026lt;-- o Commits in Git are immutable. This doesn’t mean that mistakes can’t be corrected, however; it’s just that “edits” to the commit history are actually creating entirely new commits, and references (see below) are updated to point to the new ones.\nData model, as pseudocode # // a file is a bunch of bytes type blob = array\u0026lt;byte\u0026gt; // a directory contains named files and directories type tree = map\u0026lt;string, tree | blob\u0026gt; // a commit has parents, metadata, and the top-level tree type commit = struct { parents: array\u0026lt;commit\u0026gt; author: string message: string snapshot: tree } Objects and content-addressing # An object is a blob, tree, or commit:\ntype object = blob | tree | commit In Git data store, all objects are content-addressed by their SHA-1 hash.\nobjects = map\u0026lt;string, object\u0026gt; def store(object): id = sha1(object) objects[id] = object def load(id): return objects[id] Blobs, trees, and commits are unified in this way: they are all objects. When they reference other objects, they don’t actually contain them in their on-disk representation, but have a reference to them by their hash.\nFor example, the tree for the example directory structure above (visualized using git cat-file -p 698281bc680d1995c5f4caaf3359721a5a58d48d), looks like this:\n00644 blob 4448adbf7ecd394f42ae135bbeed9676e894af85 baz.txt 040000 tree c68d233a33c5c06e0340e4c224f0afca87c8ce87 foo The tree itself contains pointers to its contents, baz.txt (a blob) and foo (a tree). If we look at the contents addressed by the hash corresponding to baz.txt with git cat-file -p 4448adbf7ecd394f42ae135bbeed9676e894af85, we get the following:\ngit is wonderful References # Now, all snapshots can be identified by their SHA-1 hashes. That’s inconvenient, because humans aren’t good at remembering strings of 40 hexadecimal characters.\nGit’s solution to this problem is human-readable names for SHA-1 hashes, called “references”. References are pointers to commits. Unlike objects, which are immutable, references are mutable (can be updated to point to a new commit). For example, the master reference usually points to the latest commit in the main branch of development.\nreferences = map\u0026lt;string, string\u0026gt; def update_reference(name, id): references[name] = id def read_reference(name): return references[name] def load_reference(name_or_id): if name_or_id in references: return load(references[name_or_id]) else: return load(name_or_id) With this, Git can use human-readable names like “master” to refer to a particular snapshot in the history, instead of a long hexadecimal string.\nOne detail is that we often want a notion of “where we currently are” in the history, so that when we take a new snapshot, we know what it is relative to (how we set the parents field of the commit). In Git, that “where we currently are” is a special reference called “HEAD”.\nRepositories # Finally, we can define what (roughly) is a Git repository: it is the data objects and references.\nOn disk, all Git stores are objects and references: that’s all there is to Git’s data model. All git commands map to some manipulation of the commit DAG by adding objects and adding/updating references.\nWhenever you’re typing in any command, think about what manipulation the command is making to the underlying graph data structure. Conversely, if you’re trying to make a particular kind of change to the commit DAG, e.g. “discard uncommitted changes and make the ‘master’ ref point to commit 5d83f9e”, there’s probably a command to do it (e.g. in this case, git checkout master; git reset --hard 5d83f9e).\nStaging area # Motivation: # For example, imagine a scenario where you’ve implemented two separate features, and you want to create two separate commits, where the first introduces the first feature, and the next introduces the second(and the first) feature.\no(original snapshot) \u0026lt;-- o(feature 1) \u0026lt;---o(feature 2) Git accommodates such scenarios by allowing you to specify which modifications should be included in the next snapshot through a mechanism called the “staging area”.\nGit command-line interface(CLI) # Basics # git help \u0026lt;command\u0026gt;: get hero for a git command git init: creates a new git repo, with data stored in the .git directory git status: tells you what’s going on git add \u0026lt;filename\u0026gt;: adds files to staging area git commit: creates a new commit Write good commit messages! Even more reasons to write good commit messages! git log: shows a flattened log of history git log --all --graph --decorate: visualizes history as a DAG git diff \u0026lt;filename\u0026gt;: show changes you made relative to the staging area git diff \u0026lt;revision\u0026gt; \u0026lt;filename\u0026gt;: shows differences in a file between snapshots git checkout \u0026lt;revision\u0026gt;: updates HEAD and current branch Branching and merging # git branch: shows branches git branch \u0026lt;name\u0026gt;: creates a branch git checkout \u0026lt;branch name\u0026gt;: switches to another branch git merge \u0026lt;revision\u0026gt;: merges into current branch git mergetool: use a fancy tool to help resolve merge conflicts git rebase: rebase set of patches onto a new base Merge Conflict: # In the below branching and merging\no(org) \u0026lt;--- o(feature 1) \u0026lt;--- o ^ / \\ v --- o(feature 2) ------\t/ # org file Line 1: Hello World Line 2: This is an example file. # feature 1 Line 1: Hello World Line 2: This is an example file. Line 3: Adding a line for clarity. # feature 2 Line 1: Hello World Line 2: This is a modified example file. If comparing to the original file, the modification are on different lines, then merging has no conflict, but imagine, two features modified a same line(e.g., both writing new line 3), there will be conflict and git will let developer to manually resolve it.\nRemote # git remote: list remotes git remote add \u0026lt;name\u0026gt; \u0026lt;url\u0026gt;: add a remote git push \u0026lt;remote\u0026gt; \u0026lt;local branch\u0026gt;:\u0026lt;remote branch\u0026gt;: send objects to remote, and update remote reference git branch --set-upstream-to=\u0026lt;remote\u0026gt;/\u0026lt;remote branch\u0026gt;: set up correspondence between local and remote branch git fetch: retrieve objects/references from a remote git pull: same as git fetch; git merge git clone: download repository from remote Others # git config: Git is highly customizable git clone --depth=1: shallow clone, without entire version history git add -p: interactive staging git rebase -i: interactive rebasing git blame: show who last edited which line git stash: temporarily remove modifications to working directory git bisect: binary search history (e.g. for regressions) .gitignore: specify intentionally untracked files to ignore TODO and future learning plan: # Workflows: we taught you the data model, plus some basic commands; we didn’t tell you what practices to follow when working on big projects (and there are many different approaches). GitHub: Git is not GitHub. GitHub has a specific way of contributing code to other projects, called pull requests. References: # Youtube Videos: https://www.youtube.com/watch?v=2sjqTHE0zok\u0026t=48s The missing semester: https://missing.csail.mit.edu/2020/version-control/ ","date":"12 August 2022","externalUrl":null,"permalink":"/posts/version-control/","section":"Blog","summary":"A brief Introduction to Version Control","title":"Version Control"},{"content":"","date":"1 January 0001","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors"},{"content":"","date":"1 January 0001","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"","date":"1 January 0001","externalUrl":null,"permalink":"/tags/misc/","section":"Tags","summary":"","title":"MISC"},{"content":"","date":"1 January 0001","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series"}]